{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Teste de hipótese.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OG3UdkFkblVc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7b-Dk-FlxVy"
      },
      "source": [
        "Perguntas\n",
        "\n",
        "*   Os atributos \"imutáveis\" (sexo, raça, país de origem...) do trabalhador são menos ou mais determinantes que os demais (como escolaridade) para predizer seu salário?  E para predizer se ele ganha acima de 50 mil dólares anuais?\n",
        "*   De modo semelhante, qual a menor combinação de atributos que melhor prediz o salário de um trabalhador? E que prediz se ele ganha mais de 50 mil dólares anuais?\n",
        "\n",
        "*   Com qual precisão conseguimos prever o salário de um trabalhador, baseado em seus atributos, a partir do dataset selecionado? E baseado apenas nos grupos de atributos das perguntas anteriores?\n",
        "\n",
        "*   Existe algum atributo que é fortemente correlacionado com outros?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WLF_T79lQwe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-colorblind')\n",
        "\n",
        "plt.rcParams['figure.figsize']  = (12, 18)\n",
        "plt.rcParams['axes.labelsize']  = 20\n",
        "plt.rcParams['axes.titlesize']  = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['lines.linewidth'] = 4\n",
        "plt.ion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCZPOrDbnea_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b2a05332-b898-42a7-b1c8-c1b29a40782d"
      },
      "source": [
        "# Load datasets\n",
        "dataset = pd.read_csv('adult_data.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f0ccd2a360b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adult_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adult_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eyl_sforTfC"
      },
      "source": [
        "# Arrumando o nome das colunas\n",
        "\n",
        "dataset.columns\n",
        "\n",
        "column_new_names = {}\n",
        "for c in dataset.columns[1:]:\n",
        "    column_new_names[c] = c.split(' ')[1].replace('-', '_')\n",
        "    \n",
        "dataset = dataset.rename(columns=column_new_names)\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MffQdRK06uz"
      },
      "source": [
        "\n",
        "# Quantidade de pessoas com salário alto e baixo\n",
        "pd.DataFrame({'count': dataset.salary.value_counts(), '%': dataset.salary.value_counts(normalize = True)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG3UdkFkblVc"
      },
      "source": [
        "# Education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICvdhI2JcyAK"
      },
      "source": [
        "## Distribuição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rStEDiwtxkc1"
      },
      "source": [
        "#Plot da Escolaridade\n",
        "column_name = 'education'\n",
        "\n",
        "new_dataset = dataset.groupby([column_name,'sex'])[\"age\"].count().reset_index(name=\"count\")\n",
        "all_marital_status = new_dataset[column_name].unique()\n",
        "\n",
        "b_width = 0.43\n",
        "\n",
        "men = list()\n",
        "women = list()\n",
        "for item in new_dataset.itertuples():\n",
        "    if item[2] == \" Male\":\n",
        "        men.append(item[3])\n",
        "    else:\n",
        "        women.append(item[3])\n",
        "\n",
        "br1 = np.arange(len(men))\n",
        "br2 = [x + b_width for x in br1]\n",
        "fig = plt.subplots(figsize=(25,20))\n",
        "\n",
        "bar1 = plt.bar(br1, men, color ='b', width = b_width,\n",
        "        edgecolor ='grey', label ='Homens')\n",
        "bar2 = plt.bar(br2, women, color ='pink', width = b_width,\n",
        "        edgecolor ='grey', label ='Mulheres')\n",
        "bar_list = [bar1, bar2]\n",
        "\n",
        "plt.xlabel('Número de Escolaridade', fontweight ='bold', fontsize = 15)\n",
        "plt.ylabel('Quantidade', fontweight ='bold', fontsize = 15)\n",
        "\n",
        "plt.xticks([r + b_width/2 for r in range(len(men))], all_marital_status, fontsize=10, ha=\"center\")\n",
        "for bars in bar_list:\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x()+ 0.07, yval + .05, yval, fontsize=12, color=\"red\", ha=\"left\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjxBvi9cpCO"
      },
      "source": [
        "## Comparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OjxN4KAbqmO"
      },
      "source": [
        "# Education-num\n",
        "\n",
        "column_name = 'education'\n",
        "secondary = 'education_num'\n",
        "#Prepara o dataset\n",
        "new_dataset = dataset.groupby([column_name,secondary , 'salary'])[\"education_num\"].count().reset_index(name=\"count\")\n",
        "# all_educations = new_dataset[column_name].unique()\n",
        "\n",
        "# print(new_dataset)\n",
        "\n",
        "# Guarda relação nome - número\n",
        "educations = dict(zip(new_dataset['education'], new_dataset['education_num']))\n",
        "print(educations)\n",
        "\n",
        "all_educations = list()\n",
        "for i in sorted(educations, key = educations.get):\n",
        "    all_educations.append(i)\n",
        "\n",
        "\n",
        "key_list = all_educations\n",
        "value_list = np.zeros(len(all_educations))\n",
        "\n",
        "education_less_dict = dict(zip(key_list, value_list))\n",
        "education_more_dict = dict(zip(key_list, value_list))\n",
        "education_all_dict = dict(zip(key_list, value_list))\n",
        "\n",
        "# display (new_dataset)\n",
        "\n",
        "for item in new_dataset.itertuples():\n",
        "    if item[3] == \" <=50K\":\n",
        "        education_less_dict[item[1]] = item[4]\n",
        "    else:\n",
        "        education_more_dict[item[1]] = item[4]\n",
        "    education_all_dict[item[1]] = education_all_dict[item[1]] + item[4]\n",
        "\n",
        "#print(education_less_dict)\n",
        "#print(education_more_dict)\n",
        "\n",
        "# Calcular fração\n",
        "fraction_less = dict(zip(key_list, value_list))\n",
        "fraction_more = dict(zip(key_list, value_list))\n",
        "for key in education_all_dict:\n",
        "    fraction_less[key] = (education_less_dict[key] / education_all_dict[key])*100\n",
        "    fraction_more[key] = (education_more_dict[key] / education_all_dict[key])*100\n",
        "\n",
        "#print(fraction_less)\n",
        "#print(fraction_more)\n",
        "\n",
        "#Configura a quantidade de barras e o posicionamento das mesmas.\n",
        "\n",
        "b_width = 0.35\n",
        "br1 = np.arange(len(fraction_less.values()))\n",
        "br2 = [x + b_width for x in br1]\n",
        "\n",
        "fig = plt.subplots(figsize=(25,10))\n",
        "\n",
        "#Cria as barras.\n",
        "bar1 = plt.bar(br1, fraction_less.values(), color ='orange', width = b_width,\n",
        "        edgecolor ='grey', label ='Porcentagem com <=50k')\n",
        "bar2 = plt.bar(br2, fraction_more.values(), color ='blue', width = b_width,\n",
        "        edgecolor ='grey', label ='Porcentagem com >50k')\n",
        "bar_list = [bar1, bar2]\n",
        "\n",
        "#Nomeia os eixos.\n",
        "plt.xlabel('Education', fontweight ='bold', fontsize = 15) #Lembrar de trocar o xlabel se for usar <<<\n",
        "plt.ylabel('Percentage', fontweight ='bold', fontsize = 15)\n",
        "\n",
        "#Coloca todas as categorias no eixo X\n",
        "plt.xticks([r + b_width/2 for r in range(len(fraction_less.values()))], all_educations, fontsize=10, ha=\"center\")\n",
        "plt.yticks(np.arange(0, 102, 2),fontsize=12)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zZAHhUqyGdK"
      },
      "source": [
        "dataset.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuKg_PncFGQ-"
      },
      "source": [
        "# Quantidade de pessoas com salário alto e baixo\n",
        "pd.DataFrame({'count': dataset.salary.value_counts(), '%': dataset.salary.value_counts(normalize = True)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xw_1nDBEECz"
      },
      "source": [
        "##Qual é a probabilidade (chance) de que ao acaso 73% dos individos com doutorado ganhem mais de 50K."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn81Jf_mzjsz"
      },
      "source": [
        "valores observado: \n",
        "\n",
        "                Doctorate:  25.907990314769975 <=50k\n",
        "                            74.09200968523002 >50k\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPVEeep1yMrI"
      },
      "source": [
        "plt.rcParams['figure.figsize']  = (16, 10)\n",
        "plt.ion()\n",
        "\n",
        "def despine(ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    # Hide the right and top spines\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "\n",
        "    # Only show ticks on the left and bottom spines\n",
        "    ax.yaxis.set_ticks_position('left')\n",
        "    ax.xaxis.set_ticks_position('bottom')\n",
        "\n",
        "def sample_proportion100(pop_size, prop, n=10000):\n",
        "    '''\n",
        "    Amostra proporções de uma população.\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    pop_size: int, tamanho da população\n",
        "    prop: double, entre 0 e 1\n",
        "    n: int, número de amostras\n",
        "    '''\n",
        "    assert(prop >= 0)\n",
        "    assert(prop <= 1)\n",
        "    \n",
        "    grupo = pop_size * prop\n",
        "    # print(grupo)\n",
        "    resultados = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        sample = np.random.randint(0, pop_size, 100)\n",
        "        resultados[i] = np.sum(sample < grupo)\n",
        "    return resultados\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olCEetdlbGKz"
      },
      "source": [
        "proporcoes = sample_proportion100(pop_size=32561, prop=0.24081)\n",
        "print(proporcoes.shape)\n",
        "bins = np.linspace(1, 100, 100) + 0.5\n",
        "plt.hist(proporcoes, bins=bins, edgecolor='k')\n",
        "plt.xlim(0, 80)\n",
        "plt.ylabel('Numero de Amostras')\n",
        "plt.xlabel('Porcentagem no grupo')\n",
        "despine()\n",
        "for item in fraction_more.values():\n",
        "    plt.plot([item], [8], 'go', ms=10)\n",
        "plt.plot([74.09], [8], 'ro', ms=10)\n",
        "# print(proporcoes)\n",
        "sample = np.random.randint(0, 32561, 10) # pega 10 inteiros entre 0 e pop_size\n",
        "print(sample)\n",
        "print(np.sum(sample < 32561*0.24081)) # ve quantos deles são menores que o esperado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xphIuy-JQBXX"
      },
      "source": [
        "np.percentile(proporcoes, 95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1svUDcu7QCY3"
      },
      "source": [
        "Com 95% de chance, no máximo 31% do grupo teria renda maior que 50K. Então, no caso de pessoas com doutorado, que o número foi 74.09, podemos concluir que ele raramente ocorrreria em uma seleção uniforme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVvl0_Ghyw0"
      },
      "source": [
        "# print(education_less_dict)\n",
        "# print(education_all_dict)\n",
        "# print(education_more_dict)\n",
        " \n",
        "def sample_proportion1000(pop_size, prop, n=10000):\n",
        "    '''\n",
        "    Amostra proporções de uma população.\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    pop_size: int, tamanho da população\n",
        "    prop: double, entre 0 e 1\n",
        "    n: int, número de amostras\n",
        "    '''\n",
        "    assert(prop >= 0)\n",
        "    assert(prop <= 1)\n",
        "    \n",
        "    grupo = pop_size * prop\n",
        "    print(\"Grupo: \",grupo)\n",
        "    resultados = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        sample = np.random.randint(0, pop_size, 1000) # pega quantos em 1000 já que mem 100 não capturava bem\n",
        "        resultados[i] = np.sum(sample < grupo)\n",
        "    return resultados\n",
        "\n",
        "prop_doct_all = education_more_dict[' Doctorate']/32561\n",
        "prop_doct_more = education_all_dict[' Doctorate']/7841\t\n",
        "print(\"Número com doc na pop com >50k:\", education_more_dict[' Doctorate'],\" Número com doc na pop total:\" ,education_all_dict[' Doctorate'])\n",
        "print(prop_doct_all,prop_doct_more)\n",
        "\n",
        "proporcoes = sample_proportion1000(pop_size=10000000, prop=prop_doct_all)\n",
        "# print(proporcoes.shape)\n",
        "bins = np.linspace(1, 100, 100) + 0.5\n",
        "plt.hist(proporcoes, bins=bins, edgecolor='k')\n",
        "plt.xlim(0, 55)\n",
        "plt.ylabel('Numero de Amostras')\n",
        "plt.xlabel('%% no grupo')\n",
        "despine()\n",
        "plt.plot(prop_doct_more*1000, [10], 'ro', ms=10)\n",
        "print(prop_doct_more*1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWlORmQHFX6u"
      },
      "source": [
        "Com 95% de chance, apenas 0,015% no grupo que ganha mais  de 50K teria doutorado. Então, como o número foi 0,0167%, podemos concluir que isso raramente ocorrreria em uma distribuição uniforme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtfo0y9xyj11"
      },
      "source": [
        "np.percentile(proporcoes, 95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvygAYMbpXH"
      },
      "source": [
        "Em um outro caso em que temos diferentes níveis de educação, podemos computar a total variation distance e fazer outra pergunta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSURwW1asjQ6"
      },
      "source": [
        "##Será que a distribuição da educação no grupo que ganha mais de 50k pode ser explicada ao acaso?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnvUU9hR0i6D"
      },
      "source": [
        "def total_variation(p, q):\n",
        "    '''\n",
        "    Computa a total variation distance com base em dois vetore, p e q\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    p: vetor de probabilidades de tamanho n\n",
        "    q: vetor de probabilidades de tamanho n\n",
        "    '''\n",
        "    return np.sum(np.abs(p - q)) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggpYPScIn9Wg"
      },
      "source": [
        "prop_doct_all = education_more_dict[' Doctorate']/32561\n",
        "prop_doct_more = education_all_dict[' Doctorate']/7841\t\n",
        "# print(\"Número com doc na pop com >50k:\", education_more_dict[' Doctorate'],\" Número com doc na pop total:\" ,education_all_dict[' Doctorate'])\n",
        "# print(prop_doct_all,prop_doct_more)\n",
        "\n",
        "idx = list()\n",
        "for i in education_all_dict.keys():\n",
        "    idx.append(i)\n",
        "print(idx)\n",
        "df = pd.DataFrame(index=idx)\n",
        "\n",
        "\n",
        "prop_pop_all = list()\n",
        "for i in education_all_dict.values():\n",
        "    prop_pop_all.append(i/32561)\n",
        "\n",
        "df['sample'] = prop_pop_all\n",
        "\n",
        "# Na amostra de mais de 50K\n",
        "prop_pop_more = list()\n",
        "for i in education_more_dict.values():\n",
        "    prop_pop_more.append(i/7841)\n",
        "\n",
        "df['pop'] = prop_pop_more\n",
        "\n",
        "\n",
        "N = 1453\n",
        "uma_amostra = []\n",
        "for g in df.index:\n",
        "    p = df.loc[g]['pop']\n",
        "    s = sample_proportion100(N, p, 1)[0]\n",
        "    uma_amostra.append(s/100)\n",
        "\n",
        "\n",
        "df['1random'] = uma_amostra\n",
        "df.plot.bar()\n",
        "plt.ylabel('Propopção')\n",
        "plt.ylabel('Grupo')\n",
        "despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVyepaGQ2GYn"
      },
      "source": [
        "total_variation(df['1random'], df['pop'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFNUai0Q2O0Q"
      },
      "source": [
        "total_variation(df['sample'], df['pop'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuHyt9S2PGq"
      },
      "source": [
        "N = 30000\n",
        "A = np.zeros(shape=(10000, len(df.index)))\n",
        "for i, g in enumerate(df.index):\n",
        "    p = df.loc[g]['pop']\n",
        "    A[:, i] = sample_proportion100(N, p) / 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8-t_HO024Uw"
      },
      "source": [
        "all_distances = []\n",
        "for i in range(A.shape[0]):\n",
        "    all_distances.append(total_variation(df['pop'], A[i])) # total_variation entra a população e "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1UD9e6F3Igh"
      },
      "source": [
        "plt.hist(all_distances, bins=30, edgecolor='k')\n",
        "plt.ylabel('Numero de Amostras de Tamanho 10k')\n",
        "plt.xlabel('Total Variation Distance')\n",
        "plt.plot([0.255407051893586], [0], 'ro', ms=15)\n",
        "despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBiVJbsHr-jw"
      },
      "source": [
        "Como histograma da TVD vemos no ponto vermelho mostra o valor no grupo que ganha mais de 50k, as barras mostram as diferentes amostras. Novamente como o valor é bastante. Rejeitamos a hipótese nula e indicamos que os dados não foram selecionados de forma uniforme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6lOVOuK3MKY"
      },
      "source": [
        "np.percentile(all_distances, 97.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AedGvoQmggAL"
      },
      "source": [
        "Testes de permutação -- Queremos demonstrar se o fato de um gênero ganhar mais que outro pode ou não ser justificado pelo acaso.\n",
        "\n",
        "Assuma uma significância de 5%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rdiGNv9gZii"
      },
      "source": [
        "def permutation_test(df, sig, rep_num, col, attribute):\n",
        "  \"\"\"\n",
        "  Parâmetros:\n",
        "  ------------\n",
        "  df: Dataframe utilizado para realizar o teste de permutação.\n",
        "  sig: Valor da significância.\n",
        "  rep_num: Quantidade de repetições do experimento.\n",
        "  col: Coluna alvo do DF para ser realizada o teste de permutação.\n",
        "  attribute: Valor da coluna alvo para a permutação ser realizada.\n",
        " \n",
        "  Retornos:\n",
        "  ------------\n",
        "  diffs: Vetor que contém a diferença dos valores analisados.\n",
        "  LI: Ponto crítico inferior.\n",
        "  LS: Ponto crítico superior.\n",
        " \n",
        "  \"\"\"\n",
        "  \n",
        "  N = rep_num\n",
        "  filtro = df[col] == attribute\n",
        "  diffs = np.zeros(N)\n",
        "  sig = sig/2\n",
        "  for i in range(N):\n",
        "    np.random.shuffle(filtro.values)\n",
        "    #df['filtro'] = np.random.permutation(df.filtro)\n",
        "    #men = df.loc[((df['salary'] == \" <=50K\") & (df['filtro'] == 1))].shape[0]\n",
        "    #women = df.loc[((df['salary'] == \" <=50K\") & (df['filtro'] == 0))].shape[0]\n",
        "    diffs[i] = df.loc[df['salary'] == ' <=50K'][filtro].shape[0] - df.loc[df['salary'] == ' <=50K'][~filtro].shape[0]\n",
        "    #diffs[i] = df[df.salary == ' <=50K'][df.filtro == 1].count().iloc[0] - \\\n",
        "    #df[df.salary == ' <=50K']['df.filtro == 0'].count().iloc[0]\n",
        "  LI = np.percentile(diffs, sig)\n",
        "  LS = np.percentile(diffs, 100-sig)\n",
        "  \n",
        "  return diffs, LI, LS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiArFd5vyvfL"
      },
      "source": [
        "sigma = 5\n",
        "rep_num = 5000\n",
        "column= \"sex\"\n",
        "at = \" Male\"\n",
        "difference_vector, LI,LS = permutation_test(dataset, sigma, rep_num,column,at)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Sn5w_d-dWh"
      },
      "source": [
        "ab = dataset.query('salary == \" <=50K\"')\n",
        "male = ab[ab.sex == ' Male'].count().iloc[0]\n",
        "female = ab[ab.sex == ' Female'].count().iloc[0]\n",
        "sample_difference = male-female\n",
        " \n",
        " \n",
        "plt.hist(difference_vector, bins=30, edgecolor='k')\n",
        "plt.ylabel('Quantidade')\n",
        "plt.xlabel('Diferença entre o número de Homens e Mulheres que ganham abaixo de 50K Dol')\n",
        "plt.plot(sample_difference, [0], 'ro', ms=15)\n",
        "#Usando np.permutation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reVX1uaIhegx"
      },
      "source": [
        "ab = dataset.query('salary == \" <=50K\"')\n",
        "male = ab[ab.sex == ' Male'].count().iloc[0]\n",
        "female = ab[ab.sex == ' Female'].count().iloc[0]\n",
        "sample_difference = male-female\n",
        " \n",
        " \n",
        "plt.hist(difference_vector, bins=30, edgecolor='k')\n",
        "plt.ylabel('Quantidade')\n",
        "plt.xlabel('Diferença entre o número de Homens e Mulheres que ganham abaixo de 50K Dol')\n",
        "plt.plot(sample_difference, [0], 'ro', ms=15)\n",
        "#Usando np.shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHA2-_6KIIEU"
      },
      "source": [
        "sigma = 5\n",
        "rep_num = 5000\n",
        "column= \"marital_status\"\n",
        "\n",
        "target_values = [' Divorced', ' Married-civ-spouse']\n",
        "df = dataset.query(\"marital_status == ' Never-married' or marital_status == ' Married-civ-spouse'\")\n",
        "\n",
        "\n",
        "#new_dataset = dataset.loc[(dataset['marital_status'].isin(target_values))]\n",
        "at = \" Married-civ-spouse\"\n",
        "diff1, li1,ls1 = permutation_test(df, sigma, rep_num,column,at)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVZxpVmHPbwP"
      },
      "source": [
        "ab = dataset.query('salary == \" <=50K\"')\n",
        "nm = ab[ab.marital_status == ' Married-civ-spouse'].count().iloc[0]\n",
        "mcs = ab[ab.marital_status == ' Never-Married'].count().iloc[0]\n",
        "sample_difference = nm-mcs\n",
        " \n",
        "plt.hist(diff1, bins=10, edgecolor='k')\n",
        "plt.ylabel('Quantidade')\n",
        "plt.xlabel('Diferença entre o número de Casados e Não Casados que ganham abaixo de 50K Dol')\n",
        "plt.plot(sample_difference, [0], 'ro', ms=15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUWlR_raDSdq"
      },
      "source": [
        "# Regressões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFpOIMSqDgTX"
      },
      "source": [
        "##Regressão Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G061qwSBDcS4"
      },
      "source": [
        "##Regressão Múltipla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M_tXNy_4Tt1"
      },
      "source": [
        "df = dataset.copy()\n",
        "df = df.replace('?', np.nan)\n",
        "df = df.dropna()\n",
        "\n",
        "df = df.drop(['fnlwgt','workclass','education','marital_status','occupation','relationship','race','sex','native_country','salary'], axis=1)\n",
        "X = df.drop('age', axis=1)\n",
        "y = df[['age']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train[['hours_per_week','education_num','capital_gain','capital_loss']], y_train)\n",
        "y_predicted = reg.predict(X_test[['hours_per_week','education_num','capital_gain','capital_loss']])\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_predicted))\n",
        "print('R²: %.2f' % r2_score(y_test, y_predicted))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test, y_predicted)\n",
        "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
        "ax.set_xlabel('measured')\n",
        "ax.set_ylabel('predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXt_4rpE2fCX"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "data = dataset.drop(columns=['fnlwgt', 'education'])\n",
        "data = data.replace('?', np.nan)\n",
        "data = data.dropna()\n",
        "data['capital'] = data['capital_gain'] - data['capital_loss']\n",
        "data = data.drop(columns=['capital_gain', 'capital_loss'])\n",
        "\n",
        "categoricalColumns = data.columns[data.dtypes==object].tolist()\n",
        "\n",
        "for col in categoricalColumns:\n",
        "    data[col] = data[col].str.strip()\n",
        "\n",
        "for col in categoricalColumns:\n",
        "    data[col] = data[col].str.replace('-', '_')\n",
        "\n",
        "data.loc[data['native_country'] != 'United_States', 'native_country'] = 'Other'\n",
        "data.loc[data['race'] != 'White', 'race'] = 'Other'\n",
        "data['salary'] = data['salary'].str.replace('<=50K', 'low')\n",
        "data['salary'] = data['salary'].str.replace('>50K', 'high')\n",
        "\n",
        "print(data.dtypes)\n",
        "print(data)\n",
        "\n",
        "data.head()\n",
        "\n",
        "formula_string_indep_vars = ' + '.join(dataset.drop(columns='hours_per_week').columns)\n",
        "formula_string = 'hours_per_week ~ ' + formula_string_indep_vars\n",
        "print('formula_string: ', formula_string)\n",
        "\n",
        "data_encoded = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "formula_string_indep_vars_encoded = ' + '.join(data_encoded.drop(columns='hours_per_week').columns)\n",
        "formula_string_encoded = 'hours_per_week ~ ' + formula_string_indep_vars_encoded\n",
        "print('formula_string_encoded: ', formula_string_encoded)\n",
        "\n",
        "#formula_string_encoded = formula_string_encoded + ' + hours_per_week:capital + race_White:capital'\n",
        "\n",
        "#formula_string_encoded = formula_string_encoded + ' + np.power(hours_per_week, 1)'\n",
        "\n",
        "model_full = sm.formula.ols(formula=formula_string_encoded, data=data_encoded)\n",
        "\n",
        "model_full_fitted = model_full.fit()\n",
        "\n",
        "print(model_full_fitted.summary())\n",
        "\n",
        "residuals_full = pd.DataFrame({'actual': data_encoded['hours_per_week'], \n",
        "                            'predicted': model_full_fitted.fittedvalues, \n",
        "                            'residual': model_full_fitted.resid})\n",
        "\n",
        "def plot_line(axis, slope, intercept, **kargs):\n",
        "    xmin, xmax = axis.get_xlim()\n",
        "    plt.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs)\n",
        "    \n",
        "# Creating scatter plot\n",
        "plt.scatter(residuals_full['actual'], residuals_full['predicted'], alpha=0.3);\n",
        "plot_line(axis=plt.gca(), slope=1, intercept=0, c=\"red\");\n",
        "plt.xlabel('Actual hours_per_week');\n",
        "plt.ylabel('Predicted Age');\n",
        "plt.title('Figure 9: Scatter plot of actual vs. predicted age for the Full Model', fontsize=15);\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBvTPdkDZ-L"
      },
      "source": [
        "##Pipeline Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lARfREDSFzra"
      },
      "source": [
        "import copy\n",
        "df = dataset.copy()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc9Wt6yJGsGF"
      },
      "source": [
        "##Separando os atributos numéricos dos categóricos, para Z normalizá-los.\n",
        "\n",
        "numerical_categories = list()\n",
        "for coluna in df.columns:\n",
        "  if (df.dtypes[coluna] == 'object'):\n",
        "    continue\n",
        "  else:\n",
        "    numerical_categories.append(coluna)\n",
        "\n",
        "print(numerical_categories)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNS6a1HrILAr"
      },
      "source": [
        "##Separando o conjunto de treino e teste para o ML.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## A coluna salário é guardada na variável y, que será o atributo previsto. A variável x guardará o resto os outros parâmetros que serão usados para prever o salário. \n",
        "## Lembrando que o dataset já veio com um arquivo de teste separado. Ignorar é prudente, pois não é possivel saber se foi retirado do adult_data, ou se corresponde\n",
        "## á um conjunto de dados adicionais. Caso seja a primeira opção, vai avacalhar tudo! :(\n",
        "\n",
        "y = df['salary']\n",
        "x = df.copy()\n",
        "x = x.drop('salary', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDxZCOUI0fe"
      },
      "source": [
        "##x_train_Znormalizado\n",
        "\n",
        "# mean_train = x_train[numerical_categories].mean()\n",
        "# std_train = x_train[numerical_categories].std(ddof=1)\n",
        "\n",
        "x_trn = x_train.copy()\n",
        "x_trn[numerical_categories] -= x_train[numerical_categories].mean()\n",
        "x_trn[numerical_categories] /= x_train[numerical_categories].std(ddof=1)\n",
        "x_trn\n",
        "##x_test_Znormalizado.\n",
        "x_ten = x_test.copy()\n",
        "x_ten[numerical_categories] -= x_test[numerical_categories].mean()\n",
        "x_ten[numerical_categories] /= x_test[numerical_categories].std(ddof=1)\n",
        "x_ten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syQM1ZHblxga"
      },
      "source": [
        "x_ten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "334vdAo0NVQ2"
      },
      "source": [
        "##Fazendo one-hot-encoding.\n",
        "x_trn = pd.get_dummies(x_trn)\n",
        "x_ten = pd.get_dummies(x_ten)\n",
        "x_trn.drop(columns=x_trn.columns[-1], inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYn1QybFNy9f"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "#Criar um conjunto de validação.. StratifiedKFold mantém a proporção original de dados do dataset nos folds criados.\n",
        "ns = 3\n",
        "skf = StratifiedKFold(n_splits=ns)\n",
        "tururu = 0\n",
        "for train_i, validation_i in skf.split(x_trn, y_train):\n",
        "  for k in [1, 3, 5, 7, 9, 11, 13, 15, 17]:\n",
        "    #Treinar o KNN\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(x_trn.iloc[train_i], y_train.iloc[train_i])\n",
        "    #Usar o KNN para prever, utilizando a parte de validação do conjunto de treino.\n",
        "    predict = model.predict(x_trn.values[validation_i])\n",
        "    original = y_train.values[validation_i]\n",
        "    #Calcular a eficiência do modelo preliminar durante os ajustes dos parâmetros 'k'.\n",
        "    for index in range(len(predict)):\n",
        "      if predict[index] == original[index]:\n",
        "        tururu +=1 \n",
        "    tururu /= len(predict)\n",
        "    print(k, tururu)\n",
        "##13 parece bom."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TdP3yeJZHdT"
      },
      "source": [
        "ns = 10\n",
        "k = 13\n",
        "skf = StratifiedKFold(n_splits=ns)\n",
        "for train_i, validation_i in skf.split(x_trn, y_train):\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(x_trn.iloc[train_i], y_train.iloc[train_i])\n",
        "    xd2 = model.score(x_trn.values[validation_i], y_train.values[validation_i])\n",
        "    lr = LogisticRegression(max_iter=10000)\n",
        "    lr.fit(x_trn.iloc[train_i], y_train.iloc[train_i])\n",
        "    xd1 =lr.score(x_trn.values[validation_i], y_train.values[validation_i])\n",
        "    print(f\"Log:{xd1}\\nKNN:{xd2}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bgi2cz21aVq"
      },
      "source": [
        "##Bootstrap\n",
        "from sklearn.metrics import accuracy_score\n",
        "def bootstrap(x, y, model, n=5000):\n",
        "  size = len(x)\n",
        "  values = np.zeros(n)\n",
        "  idx = np.arange(len(y))\n",
        "  for i in range(n):\n",
        "    sample = np.random.choice(idx, size=size, replace=True)\n",
        "    values[i] = model.score(x.values[sample], y.values[sample])\n",
        "  return values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGj-o9FMkVbR"
      },
      "source": [
        "##LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "lr.fit(x_trn, y_train)\n",
        "\n",
        "precisao_final = lr.score(x_ten, y_test)\n",
        "print(precisao_final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyzWHRyXoVaY"
      },
      "source": [
        "boots = bootstrap(x_ten, y_test, lr)\n",
        "print(boots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ_vRrNhzIHp"
      },
      "source": [
        "LL = np.percentile(boots, 5)\n",
        "RL = np.percentile(boots,95)\n",
        "print(f\"O intervalo de confiança da regressão logística é: [{LL}, {RL}]\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}